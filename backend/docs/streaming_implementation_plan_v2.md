# Voice Mode Streaming Implementation Plan (v2)

## Overview
Implement separate endpoints for text and voice modes, ensuring all frontend interactions use the auto-generated OpenAPI client.

## Critical Constraint
⚠️ **All frontend-backend communication MUST go through the auto-generated OpenAPI client**
- No direct WebSocket connections from frontend
- No manual fetch calls
- Use generated types and methods only

## Current State
- **Text Mode**: Working well with SSE + POST through generated client
- **Voice Mode**: Using same endpoints, not optimized for real-time audio

## Problem with WebSockets
WebSockets are not well-supported by OpenAPI 3.0 and auto-generated clients. This creates a challenge for real-time bidirectional streaming.

## Revised Approach: Server-Sent Events (SSE) for Both Directions

### Architecture Overview
Since we need to use the auto-generated client, we'll implement a clever SSE-based approach:

1. **Voice Stream Initiation**: 
   - `POST /api/voice/sessions` - Create a voice session
   - Returns session ID and upload URL

2. **Bidirectional Communication**:
   - **Client→Server**: `POST /api/voice/sessions/{session_id}/audio` - Send audio chunks
   - **Server→Client**: `GET /api/voice/sessions/{session_id}/stream` - SSE for responses

3. **Control Signals**:
   - `POST /api/voice/sessions/{session_id}/control` - Send control commands

### Implementation Details

#### 1. Voice Session Management
```python
@app.post("/REDACTED)
async def create_voice_session(request: CreateVoiceSessionRequest):
    """Create a new voice streaming session."""
    # Create ADK session with streaming capabilities
    # Return session details
```

#### 2. Audio Upload Endpoint
```python
@app.post("/api/voice/sessions/{session_id}/audio")
async def upload_audio_chunk(session_id: str, audio: AudioChunkRequest):
    """Upload audio chunk to ongoing session."""
    # Feed audio to ADK stream
    # Process through run_async_stream
```

#### 3. SSE Stream Endpoint
```python
@app.get("/api/voice/sessions/{session_id}/stream")
async def voice_stream_sse(session_id: str):
    """SSE endpoint for voice responses."""
    # Stream ADK responses
    # Include transcripts and audio
```

### Frontend Integration

The auto-generated client will provide methods like:

```typescript
// Generated by OpenAPI client
interface ApiClient {
  // Create voice session
  createVoiceSession(request: CreateVoiceSessionRequest): Promise<VoiceSessionResponse>
  
  // Send audio chunks
  uploadAudioChunk(sessionId: string, audio: AudioChunkRequest): Promise<void>
  
  // SSE stream (returns EventSource)
  createVoiceEventSource(sessionId: string): EventSource
  
  // Control commands
  sendVoiceControl(sessionId: string, control: ControlRequest): Promise<void>
}
```

### Updated Frontend Flow

```typescript
// use-natural-conversation.ts modifications
async function startVoiceSession() {
  // 1. Create session through generated client
  const session = await ApiClient.createVoiceSession({
    language: 'en-US'
  })
  
  // 2. Set up SSE for responses
  const eventSource = REDACTED(session.sessionId)
  
  // 3. Send audio chunks through generated client
  // Instead of WebSocket, use periodic uploads
  const sendAudio = async (audioData: ArrayBuffer) => {
    await ApiClient.uploadAudioChunk(session.sessionId, {
      data: arrayBufferToBase64(audioData),
      timestamp: Date.now()
    })
  }
}
```

## API Schema Updates

Add to OpenAPI schema:

```yaml
/api/voice/sessions:
  post:
    summary: Create voice session
    operationId: createVoiceSession
    requestBody:
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/CreateVoiceSessionRequest'
    responses:
      200:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/VoiceSessionResponse'

/api/voice/sessions/{session_id}/audio:
  post:
    summary: Upload audio chunk
    operationId: uploadAudioChunk
    parameters:
      - name: session_id
        in: path
        required: true
        schema:
          type: string
    requestBody:
      content:
        application/json:
          schema:
            $ref: '#/components/schemas/AudioChunkRequest'

/api/voice/sessions/{session_id}/stream:
  get:
    summary: Voice response stream
    operationId: getVoiceStream
    parameters:
      - name: session_id
        in: path
        required: true
        schema:
          type: string
    responses:
      200:
        description: Server-sent events stream
        content:
          text/event-stream:
            schema:
              type: string
```

## Benefits of This Approach

1. **Full OpenAPI Compatibility**: All endpoints work with auto-generated clients
2. **Type Safety**: Full TypeScript types for all interactions
3. **Separation of Concerns**: Voice and text modes have distinct endpoints
4. **Streaming Support**: SSE provides real-time responses
5. **No Custom Code**: Frontend only uses generated client methods

## Implementation Steps

1. **Backend**:
   - Create new voice session endpoints
   - Implement ADK streaming integration
   - Add proper OpenAPI annotations

2. **OpenAPI Schema**:
   - Define new endpoints and models
   - Regenerate client

3. **Frontend**:
   - Update `use-natural-conversation.ts` to use new endpoints
   - Use only generated client methods
   - Remove any direct API calls

## Alternative: Long-Polling Approach

If SSE + chunked uploads still has too much latency, consider:

1. **Persistent Connection Session**:
   - `POST /api/voice/sessions/{id}/connect` - Long-polling endpoint
   - Holds connection open for bidirectional streaming
   - Still works with generated clients

2. **Chunked Transfer Encoding**:
   - Use HTTP chunked responses for streaming
   - Compatible with OpenAPI clients

## Migration Path

1. Keep existing endpoints working
2. Implement new voice endpoints
3. Update frontend to use new endpoints via generated client
4. Test thoroughly
5. Remove old audio support from text endpoints

## Success Criteria

- All frontend code uses generated client only
- Voice latency < 500ms (realistic with HTTP)
- Clean separation between text and voice
- Full type safety maintained